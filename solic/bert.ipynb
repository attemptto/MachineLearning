{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from random import *\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils import data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n' # R\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
    "    'Nice meet you too. How are you today?\\n' # R\n",
    "    'Great. My baseball team won the competition.\\n' # J\n",
    "    'Oh Congratulations, Juliet\\n' # R\n",
    "    'Thank you Romeo\\n' # J\n",
    "    'Where are you going today?\\n' # R\n",
    "    'I am going shopping. What about you?\\n' # J\n",
    "    'I am going to visit my grandmother. she is not very well' # R\n",
    ")\n",
    "sentences = re.sub(\"[.,!?\\\\-]\",' ',text.lower()).split('\\n')\n",
    "print(np.array(sentences).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_2_index = {'[PAD]':0,'[CLS]':1,'[SEP]':2,'[MASK]':3}\n",
    "#构建语料库\n",
    "for i,w in enumerate(word_list):\n",
    "    word_2_index[w] = i + 4\n",
    "index_2_word = {i: w for i,w in enumerate(word_2_index)}\n",
    "vocab_size = len(word_2_index)\n",
    "print(index_2_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word_2_index[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "batch_size = 6\n",
    "max_pred = 5 # max tokens of prediction\n",
    "n_layers = 6\n",
    "n_heads = 12\n",
    "d_model = 768\n",
    "d_ff = 768*4 # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive!=batch_size/2 or negative!=batch_size/2:\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)),randrange(len(sentences))\n",
    "        tokens_a,tokens_b = token_list[tokens_a_index],token_list[tokens_b_index]\n",
    "\n",
    "        # cls的index + token_A句子 + sep的index + token_B 句子 + sep的index\n",
    "        input_ids = [word_2_index['[CLS]']]+tokens_a+[word_2_index['[SEP]']]+tokens_b+[word_2_index['[SEP]']]\n",
    "\n",
    "\n",
    "        # CLS(0)+第一句[0]+SEP(0)+第二句[1]+SEP(1)\n",
    "        segment_ids = [0] * (1 + len(tokens_a)+ 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM\n",
    "        n_pred = min(max_pred,max(1,int(len(input_ids)*0.15)))\n",
    "\n",
    "        # 候选mask的位置\n",
    "        cand_maked_pos = [i for i,token in enumerate(input_ids) if token!=word_2_index['[SEP]'] and token!=word_2_index['[CLS]'] ]\n",
    "\n",
    "        shuffle(cand_maked_pos)\n",
    "\n",
    "        masked_tokens, masked_pos = [], []\n",
    "\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            #input_id 对应位置的值就是token\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random()<0.8:\n",
    "                input_ids[pos] = word_2_index['[MASK]']\n",
    "            elif random()>0.9:\n",
    "                index = randint(0, vocab_size-1)\n",
    "\n",
    "                while index<4:\n",
    "                    index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = index\n",
    "\n",
    "        #零填充\n",
    "        #未达到最大长度的进行填充\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * int(n_pad))\n",
    "            masked_pos.extend([0] * int(n_pad))\n",
    "\n",
    "        if tokens_a_index+1 == tokens_b_index and positive<batch_size/2:\n",
    "            batch.append([input_ids,segment_ids,masked_tokens,masked_pos,True])\n",
    "            positive +=1\n",
    "        elif tokens_a_index+1 !=tokens_b_index and negative<batch_size/2:\n",
    "            batch.append([input_ids,segment_ids,masked_tokens,masked_pos,False])\n",
    "            negative +=1\n",
    "\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "batch = make_data()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
    "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
    "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataloader.DataLoader at 0x2088a5180b8>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class myDataset(data.Dataset):\n",
    "    def __init__(self,input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens  = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.isNext= isNext\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.segment_ids[index], self.masked_tokens[index], self.masked_pos[index], self.isNext[index]\n",
    "\n",
    "loader = data.DataLoader(myDataset(input_ids,segment_ids,masked_tokens,masked_pos,isNext),batch_size,True)\n",
    "loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}